---
description: 컴퓨터는 어떻게 우리가 하는 일들을 이해하고 처리하는 걸까?
---

# 컴퓨터 언어 체계 이해하기

<figure><img src=".gitbook/assets/mac.jpeg" alt=""><figcaption></figcaption></figure>





### 배경 <a href="#undefined" id="undefined"></a>

역사를 돌이켜보면, 인간은 의사소통하기 위해 언어를 사용해왔습니다. 전 세계에 많은 언어가 존재하지만, 의사소통이 가능한 이유는 지칭하는 어떤 것의 문맥이 동일하기 때문입니다. 예를 들어, ‘TIME’과 ‘시간’처럼 영어와 한글의 모습은 다르지만, 의미는 같은 것을 문맥이라고 말할 수 있겠네요.&#x20;

컴퓨터가 사용하는 언어, 즉 기계어 또한 인간이 만들어낸 컴퓨터 언어에 불과합니다. 사람들은 기계와 자연어가 같은 문맥을 사용한다면 충분히 의사소통이 가능할 거로 생각했고 오랜 시간에 걸쳐 이를 되게끔 했습니다. 오랜 시간 동안 언어가 진화해왔던 것처럼 인간은 끝없이 문제를 해결했고 오늘날의 컴퓨터 언어를 만들었습니다.

아시다시피, 우리가 사용하는 언어(자연어)는 기호 혹은 자음과 모음으로 문자를 구성합니다. 한자리의 표현을 상자에 담는다고 표현한다면, 컴퓨터는 비트(_binary digit_)라고 부르는 2진 숫자를 사용하여 아주 적은 비용으로 표현하고자 하는 것을 상자에 담을 수 있답니다. 10진수를 사용하는 자연어와 달리, <mark style="color:blue;">**컴퓨터가 이러한 체계를 사용하는 이유는 트랜지스터에 전원을 공급하여 스위치를 켜고 끄는 이 일련에 작업 매우 단순하기 때문**</mark>이에요. 비트를 통해 우리가 평소 사용하는 텍스트, 이모티콘, 동영상, 소리 그리고 더 복잡한 데이터를 표현할 수 있게 되었습니다.







### **컴퓨터의 논리연산**

비트는 1과 0만을 사용할 수 있는 2진법이지만 논리연산과 _NOT, AND, OR_ 연산으로 계산할 수 있습니다. 또한 _XOR_ 과 드모르간 법칙을 적용할 수 있습니다. _XOR_ 은 서로 다른 논리의 계산을 _TRUE_ 로 반환, 같은 값은 _FALSE_ 로 반환하는 계산식입니다. 드모르간 법칙은 수학자 드모르간이 발견한 법칙으로 _a AND b는 NOT(NOT a OR NOT b)_ 과 같다는 것을 증명합니다.&#x20;

흥미로운 것은 XOR과 드모르간 법칙의 증명으로 연산은 다른 연산으로 치환할 수 있다는 것입니다. _AND_ 는 _OR_로 변경할 수 있고 _XOR_도 _NOT, AND, OR_ 의 조합으로 바꿀 수 있습니다. 컴퓨터로 <mark style="color:blue;">**작업하다 보면 원하는 값을 인풋으로 못 받는 경우가 분명히 있기 때문**</mark>에 이러한 성질은 매우 유용합니다.

{% hint style="warning" %}
_"부정논리, NOT연산을 연속으로 쓰는 것은 효율성이 많이 떨어집니다. 연산을 추가적으로 사용하기 때문에 최소한으로 사용하는 것이 좋습니다. 또한 전달력에서도 좋은 방법은 아닙니다. 야식 안먹을려고 했는데 안먹을 수가 없었다는 야식 먹었다는 소리자나요 ㅎㅎ 동등한 연산이면 간단하게 계산하는 것이 여러방면에서 좋습니다."_
{% endhint %}

__

__

__

### **컴퓨터의 정수표현**

수는 논리보다 더 복잡하지만, 단어보다는 훨씬 단순합니다. 우리는 보통 10진수 체계를 사용하여 10가지 기호의 한자리를 표현합니다. 1000(103 + 102 + 101 + 100)처럼 표현하는 것을 base10 시스템이라 말합니다. 비트를 사용해 값을 만들 때도 이와 비슷하게 접근할 수 있습니다. 2진수에서는 기호가 2개이기 때문에 각 상자의 자릿수는 2의 거듭 제곱이며, 따라서 2진수 체계는 10을 밑으로 하지 않고 2를 밑으로 하는 시스템입니다.

$$
1001110100100 =1*2**12 + 0*2**11 + ... + 0*2**0 =5,028
$$

5028을 2진 체계로 표현했을 때 13개의 비트가 사용 되었습니다. 하지만 실제 컴퓨터에서는 0001001110100100의 모습으로 보이는데 그 이유는 컴퓨터가 미리 정해진 수의 비트를 한 덩어리로 사용하도록 만들어졌기 때문입니다. 그래서 항상 일정한 개수의 비트를 사용해 값을 표현하는 경우가 있습니다. 이때 유효하지 않은 비트에 0을 리딩제로라하고 가장 오른쪽의 비트를 LSB, 왼쪽 비트를 MSB라 축약하여 부르기도 합니다.

{% hint style="info" %}
_"10진수를 2진수로 표현하는 또다른 방법은 2진 코드화한 10진수(BCD)로 변환하는 것입니다. 예를들어, 12를 2진수로 표현하면 1100이지만 BCD를 이용하면 0001 0010로 나타낼 수 있습니다. 하지만 이 방법은 4비트로도 충분히 나타낼 수 있는 수를 8비트로 표현하기에 배우 비효율적이라 이런 시스템은 잘 사용하지 않습니다."_
{% endhint %}



****

****

### **컴퓨터의 실수표현**

밑이 10인 실수에는 10진 소수점이 포함됩니다. 밑이 2인 경우에도 2진 소수점을 표현할 방법이 필요합니다. 이전에 여러가지 방법으로 음수를 표현했던 것처럼 문맥에 따라 실수를 표현하는 방법이 달라질 수 있습니다.

첫 번째 실수 표현 방법 4비트 중에서 2비트는 정수를 나타낼 때 사용하고 나머지 2비트는 실수를 표현하는데 사용합니다. 이 경우, 소수점의 위치가 항상 일정하기 때문에 이 방식을 **고정 소수점 표현법**이라합니다.

{% hint style="info" %}
_" 10진 체계에서는 1/10, (1/10)\*\*2, ... , (1/10)\*\*n 사용하지만 2진 체계에서는 2의 거듭제곱을 분모로 사용합니다. 1/2, (1/2)\*\*2, ... , (1/2)\*\*n "_
{% endhint %}

4비트를 실수를 표현하는데 사용한다면 분수 부분을 표현하는 방법은 4가지가 될 수 있습니다. 이런 접근 방법이 잘 작동하긴 하지만 00(0) ,01(1/4), 10(1/2), 11(3/4)의 구성으로 <mark style="color:blue;">**쓸모있는 실수를 표현하기는 제한적**</mark>일 수 밖에 없습니다.&#x20;

일반적인 문제를 해결하기하려면 넓은 범위의 수를 다룰 수 있어야합니다. 예를 들어, 물리학에서 사용하는 범위인 플랑크 상수에서부터 아보가르드로 수까지의 실수표현을 위해서는 대략 191비트가 필요합니다. 모든 수를 몇백 비트로 표현하면 메모리 비용이 너무 많이들기때문에 고정소수점은 적절한 표현방법이 아닙니다.

다른 방법은 **실수에 과학적 표기법을 적용하는 것입니다. 이 방법은 0.0012처럼 모든 자리의 0의 자리를 표현하는 것이 아니 1.2\*10^3처럼 사용하는 것입니다. 지수를 바꾸어 소수점을 이동시키는 이 방법을 부동소수점 표현법이라고 합니다.** 4비트를 사용할 때, 2자리는 소수점 한자리를 표현하는 가수를 위해 사용하고 나머지 2자리는 지수(2^n)로 사용합니다.&#x20;

$$
0110 = 1/2*2^2 = 2.0 , 1001 = 1*2^1 = 2.0,   1110 = 1/2 * 2^2 = 6.0
$$

​부동 소수점을 0000\~1111값을 표현했을 때 아주 비효율적인 것을 확인할 수 있습니다. 예를 들어 0을 표현하는 방법은 4가지나 되고 1.0, 2.0, 4.0을 표현하는 방법도 두가지씩있습니다. 또한 지수가 커질 수록 가수의 한 패턴과 다른 패턴 사이의 값의 차이가 커집니다.

사실 부동소수점 수 시스템은 컴퓨터에서 계산을 수행할 때 실수를 표현하는 표준방법입니다. 예시보다 훨씬 더 많은 비트를 사용하며 가수와 지수에 대해 각각 부호 비트를 사용합니다. IEEE 754라는 표준은 앞에 다뤘던 비효율적인 부분을 최소화하기 위한 다양한 기능을 정의합니다. 대표적으로 정밀도를 높이는 정규화가 있습니다.

더욱 넓은 범위를 표현하기 위해서 2가지 방법이 사용되는데 하나는 32비트를 사용하는 기본정밀도, 62비트를 사용하는 더블 정밀도 부동소수점 수입니다. 몇가지 특징에 대해서 살펴보면, 두배 정밀도는 기본 정밀도에 비해서 지수는 3비트가 더 크며 가수는 무려 29비트 더 큽니다. 지수의 부호를 표현하는 값은 따로 없지만 편향된 지숫값을 사용하여 이를 표현합니다. 또한 0으로 나눴을 때 양의 무한대 또는 음의 무한대를 나타내는 여러가지 특별한 비트패턴을 제공합니다.

{% hint style="info" %}
_" NaN(not a number) 또한 IEEE754에서 제공하는 특별한 값입니다. 부동소수점 수로 계산하던 도중에 NaN이 생기면 잘못된 산술연산을 수행했다는 뜻입니다. 이러한 특별한 비트 패턴들은 특별한 지숫값을 사용합니다. 지수의 비트가 모두 0또는 1일때 특별한 의미를 갖게 하고 편향된 지수값을 사용하였습니다. 이 의미는 지수값의 범위 0\~254의 표현을 -126 \~ +127범위 표현하게 함으로서 지수의 부호가 필요없게 되었습니다._
{% endhint %}

__

_****_

_****_

### **컴퓨터의 문자표현**

예전에는 문자를 코드로 표현할 때 한 바이트안에 들어가도록 표현했습니다. 수와 마찬가지로 문자를 표현하는 방법의 경우에도 몇 가지 방법이 있습니다. 가장 초기에는 미국 표준 코드(ASCII)가 모든 기호에 대해 7비트 수 값을 할당했습니다. 65(A)\~90(Z), 97(a)\~122(z)을 보면 알파벳을 표현하기에는 충분했습니다. 상당 기간 표준 역할을 했지만 컴퓨터가 널기 쓰이게 됨에 따라 그 밖의 언어를 지원해야 할 필요성이 점차 늘어나게 되었습니다.

국제 표준화 기구(ISO)는 아스키를 확장하여 다른 나라의 문자를 표현하기위해 다양한 표준을 도입했습니다. 이렇게 각기다른 표준이 존재하는 이유는 비트가 지금보다 더 비싼 시절에 표준이 만들어 졌었고 모두 7,8비트에 욱여넣을 수 밖에 없었기 때문입니다. 비트 가격이 떨어짐에 따라 유니코드라는 새로운 표준이 만들어졌고, 문자에 16비트 코드를 부여했습니다. 하지만 현재에는 그것마저 부족하여 앞으로 모든 문자를 표현하기위해 21비트로 확장되었습니다.

지금까지는 숫자체계를 사용하여 문자를 표현하였습니다. 반대로 문자를 사용하여 수를 표현할 수 있습니다. 베이스64인코딩은 3바이트 데이터를 4문자로 표현합니다. 이렇게 표현하였을 때 특정 문자들의 기능덕분에 다양한 표현이 가능하게 되었습니다. 만일 기능적으로 사용하지않고 문자의 의미 그대로 나타내려면 어떻게 해야할까요? URL인코딩은 %뒤에 어떤 문자의 16진 표현을 덧붙이는 방식으로 기능을 제거한 문자를 인코딩합니다. 예를 들어, 문자 ' / ' 는 %2F로 대신하고 '%' 는 %25로 나타냅니다.

{% hint style="info" %}
_" 가장 많이 쓰는 아스키코드의 경우 8비트면 충분한데 굳이 16비트를 사용해야하는 걸까? UTF-8 인코딩은 데이터의 아스키 문자를 8비트로 변환하고 아스키를 받아서 처리하는 프로그램이 깨지지 않는 방법으로 문자를 인코딩합니다. 아스키 데이터를 인코딩할 때 추가적인 공간을 사용하지 않는 방법입니다._
{% endhint %}
